<!DOCTYPE HTML>
<html lang="en">
	<head>
  
		

		<title>DLRC 2018 winners! · argmax.ai</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
	
		<link rel="stylesheet" href="https://argmax.ai/theme/css/main.css" />
		<link rel="stylesheet" href="https://argmax.ai/theme/css/pygment.css" />
		<link rel="stylesheet" href="https://argmax.ai/theme/css/custom.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="https://argmax.ai/theme/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="https://argmax.ai/theme/css/ie8.css" /><![endif]-->


		<style>
		body {
		    background: #fffaff;
		}
		</style>
		


		<script src="https://argmax.ai/theme/js/jquery.min.js"></script>


		<link rel="shortcut icon" href="https://argmax.ai/images/iconified/favicon.ico" type="image/x-icon" />
		<link rel="apple-touch-icon" href="https://argmax.ai/images/iconified/apple-touch-icon.png" />
		<link rel="apple-touch-icon" sizes="57x57" href="https://argmax.ai/images/iconified/apple-touch-icon-57x57.png" />
		<link rel="apple-touch-icon" sizes="72x72" href="https://argmax.ai/images/iconified/apple-touch-icon-72x72.png" />
		<link rel="apple-touch-icon" sizes="76x76" href="https://argmax.ai/images/iconified/apple-touch-icon-76x76.png" />
		<link rel="apple-touch-icon" sizes="114x114" href="https://argmax.ai/images/iconified/apple-touch-icon-114x114.png" />
		<link rel="apple-touch-icon" sizes="120x120" href="https://argmax.ai/images/iconified/apple-touch-icon-120x120.png" />
		<link rel="apple-touch-icon" sizes="144x144" href="https://argmax.ai/images/iconified/apple-touch-icon-144x144.png" />
		<link rel="apple-touch-icon" sizes="152x152" href="https://argmax.ai/images/iconified/apple-touch-icon-152x152.png" />
		<link rel="apple-touch-icon" sizes="180x180" href="https://argmax.ai/images/iconified/apple-touch-icon-180x180.png" />


  <meta property="og:title" content="DLRC 2018 winners! - argmax.ai"/>
  <meta property="og:description" content="Introduction The Deep Learning Research Challenge centres around teaching a Franka Emika Panda 7DoF Robotic Arm to classify sensor readings according to SELF, ENVIRONMENT and OTHER. The Panda is equipped with an Intel RealSense depth camera mounted adjacent to the end effector. It captures RGB images at 480 x 640 …"/>
  <meta property="og:url" content="https://argmax.ai/showmenot/dlrc-18-winners/" />



    <meta name="tags" contents="DLRC" />

  <script src="https://apis.google.com/js/platform.js" async defer></script>
  <script src="https://argmax.ai/theme/js/custom.js"></script>


	</head>
	<body class="single">
      <progress value="0"></progress>
	<!-- Wrapper -->
		<div id="wrapper">

			<!-- Header -->
				<header id="header">
					<h1><a href="https://argmax.ai/">argmax.ai</a></h1>
					<nav class="links">
	
						<ul>
							<li><a href="https://argmax.ai/papers">Papers</a></li>

							<li><a href="https://argmax.ai/blog/">blog</a></li>
							<li><a href="https://argmax.ai/ml-course/">ml course</a></li>
							<li><a href="https://argmax.ai/talks/">talks</a></li>
							<li><a href="https://argmax.ai/team/">team</a></li>

						</ul>

					</nav>
					
					<nav class="main">
						<ul>
							<!-- Social -->

							<li class="menu">
								<a class="fa-edit" href="https://gitlab.com/-/ide/project/argmax-ai/misc/argmax_blog_source/edit/master/-/content/blog/2018-12-06_dlrc_18_winners.md">Edit</a>
							</li>

							<li class="search">
								<a class="fa-search" href="#search">Search</a>
								<form id="search" method="get" action="https://argmax.ai/search.html">
									<input type="text" name="q" placeholder="Search" id="tipue_search_input" autocomplete="off" required />
								</form>
							</li>
							<li class="menu">
								<a class="fa-bars" href="#menu">Menu</a>
							</li>
						</ul>
					</nav>
				</header>

			<!-- Menu -->
<section id="menu">

  <!-- Search -->
    <section>
      <form class="search" method="get" action="https://argmax.ai/search.html">
        <input type="text" name="q" id="tipue_search_input" placeholder="Search" autocomplete="off" required />
      </form>
    </section>

  <!-- Links -->
    <section>
      <ul class="links">



        <li>
          <a href="https://argmax.ai/papers">
            <h3>Papers</h3>
<p>Our latest publications</p>          </a>
        </li>

        <li>
          <a href="https://argmax.ai/blog/">
            <h3>blog</h3>
<p>Implementations, papers, opinions, etc.</p>          </a>
        </li>
        <li>
          <a href="https://argmax.ai/ml-course/">
            <h3>ml course</h3>
<p>Video lectures and slides on Machine Learning</p>          </a>
        </li>
        <li>
          <a href="https://argmax.ai/talks/">
            <h3>talks</h3>
<p>Open scientific talks</p>          </a>
        </li>
        <li>
          <a href="https://argmax.ai/team/">
            <h3>team</h3>
<p>The whole crew</p>          </a>
        </li>


      </ul>
    </section>

  <!-- Social -->

<!-- 
    <section>
      <ul class="align-center actions horizontal">
        <li><a href="None" class="icon fa-feed"><span class="label">RSS</span></a></li>
      </ul>
    </section>
 -->

  <!-- Actions -->
<!--  -->
    <section>
      <ul class="actions vertical">
        <li><a href="https://argmax.ai/about" class="button big fit">About</a></li>
      </ul>
    </section>

</section>
			<!-- Main -->
  <article class="post">
    <p>
    This blog details the work completed by Emma Pabich from Aachen, Ewoud Crolle from Delft and Karen Archer from Hatfield for DLRC 2018, which took place in September/October 2018.  This challenge, initiated by the Volkswagen Group ML Research Lab, is an annual event that invites students to join for a month and work on a scientific problem combining machine learning and robotics.<br>The contents of this post are not related to the research work done at https://argmax.ai.
    </p>
  </article>
<div id="main">
  <article class="post">
    <header>
      <div class="title">
        <h2>
        DLRC 2018 winners!
        </h2>
        <p>They describe their approach</p>
      </div>
      <div class="meta">
        <time class="published" datetime="2018-12-06T00:00:00+01:00">06 December 2018</time>



        <a class="author" href="https://argmax.ai/">
        <span class="name">argmax</span>
        <img src="https://argmax.ai/images/avatars/team_mini.png" alt="" />
        </a>
      </div>
    </header>

     
    <p >
        <h2 id="introduction">Introduction</h2>
<p>The Deep Learning Research Challenge centres around teaching a Franka Emika Panda 7DoF Robotic Arm to classify sensor readings according to <code>SELF</code>, <code>ENVIRONMENT</code> and <code>OTHER</code>. </p>
<p>The Panda is equipped with an Intel RealSense depth camera mounted adjacent to the end effector. It captures RGB images at 480 x 640 pixels and depth information at 240 x 320 locations in the field of view of the camera. The upper limit of depth detection is 10m, with accuracy dropping off at short range and towards 10m. Given the lab environment, depth information has typically been thresholded to the region we are interested in.</p>
<!-- PELICAN_END_SUMMARY -->

<p>In addition the Panda is also equipped with nine LiDAR single point sensors distributed equally on the fourth and sixth joint. These LiDAR sensors have a range of approximately 2m. If nothing is detected within this range, a value of 8191 is received. Throughout the project, these have been interpreted to mean no object detected within 2m, and therefore readings were capped at a value of 2m.</p>
<figure class="cap-left">    
  <img class="image fit" src='https://argmax.ai/images/dlrcpandas/robot.JPG' alt='robot'>
  <figcaption> 
    Picture showing location of LiDAR sensors and RGB-D camera.
  </figcaption>
</figure>

<p>The classification of sensor readings into <code>SELF</code>, <code>ENVIRONMENT</code> and <code>OTHER</code> agents can have multiple purposes, including to complete a task in the working environment, avoid obstacles while performing that task or even detecting another robot in a collaborative setting. As an additional task we could choose our own "brownie task".</p>
<p>LiDAR classification into <code>SELF</code> has been achieved using a supervised approach, supplemented with an additional neural network to predict the expected mean LiDAR reading for a given joint configuration. For the RGB and depth information we have used another supervised approach, built on the convolutional encoder decoder SegNet <a href='#badrinarayanan2015segnet' id='ref-badrinarayanan2015segnet-1'>(Badrinarayanan et al., 2015)</a> which segments sensor output according to <code>SELF</code> using both the RGB and depth information, making the classification more robust then only using the RGB or the depth information separately. The classification into <code>OTHER</code> agents has been done by detecting motion in the environment using the pretrained neural network Flownet2 <a href='#ilg2017flownet' id='ref-ilg2017flownet-1'>(Ilg et al., 2017)</a></p>
<p>As a brownie task we have elected that the robot clear up coloured Jenga blocks from the table and put them in a box. The rectangular blocks are identified using simple colour masking and the depth information is used to estimate location, size and orientation to enable the robot to complete the task.</p>
<p>We have two possibilities to move towards the blocks, first by the internal controller and then secondly by our implemented torque controller which can do collision avoidance while moving to an object. This collision avoidance is done by probabilistic task prioritisation.</p>
<h2 id="classifying-lidar-sensors">Classifying LiDAR Sensors</h2>
<p>There are nine single LiDAR sensors mounted on the Panda, on two of the upper joints, facing outwards typically symmetrical and perpendicular to the joint. The LiDAR reading supplies a relative distance from the sensor itself in a certain direction towards a maximum distance of 2m. Hence, the LiDAR sensors provide us a measure of distance, primarily used to gather real time information about the environment for the sake of collision avoidance. As a single measurement, it can not be used for recognising what the object is. This gives rise to one problem: some of the sensors give a close by reading when it detects a part of the robot itself. Acting on this reading as if there was an obstacle would cause the robot to try to move away from itself resulting in erratic behaviour.</p>
<table>
  <tr>
    <th>
        <video width=100% controls>
          <source src="https://argmax.ai/images/dlrcpandas/crazyrobot.m4v" type="video/mp4">
        </video>        
    </th>
  </tr>
  <tbody>
    <tr>
      <td>Crazy Panda avoiding itself in simulation resulting in sporadic behaviour.</td>
    </tr>
  </tbody>
</table>

<h3 id="transformation-to-world-frame">Transformation to World Frame</h3>
<p>Because the LiDARs are mounted on the robot itself, the position and orientation changes with the motion of the robot. The position of each joint in cartesian coordinates can be calculated from the current angle of each joint using transformations and the Denavit-Hartenberg parameters supplied by the manufacturer. Additional transformations were applied to estimate the position of the LiDAR sensors by measuring the offset of each LiDAR compared to its adjacent joint. We estimated the direction of each LiDAR according to the axis of the coordinate system of the attached joint. Using this information LiDAR readings were calculated in the 3D real world coordinates relative to the base frame of the robot.</p>
<h3 id="virtual-walls">Virtual Walls</h3>
<p>During normal operation of the robot, the end effector of the robot was constrained to remain within 'virtual walls', essentially a box directly in front of the robot arm. These constraints were adhered to when collecting data and later designing a suitable task.</p>
<h3 id="supervised-classification-of-self">Supervised Classification of Self</h3>
<p>An analytical solution of the space occupied by the robot would be intractable given the dependency on the joint angles and complicated curves and geometry of the Panda. An approximation would be to model the links as cylinders, however this would be time consuming without any guarantees of sufficient accuracy. Knowing the position and direction of LiDAR sensors in the world frame we favoured an approach of developing heuristics to establish whether a LiDAR was directed at itself. Similiar heuristics were used when considering output from the depth camera as discussed later.</p>
<p>These heuristics enabled us to develop supervised classification of the LiDAR sensors using the joint angles as input. Each LiDAR reading was then classified as to whether the Panda sees itself or not. We were therefore able to automatically label sufficient LiDAR data to train neural networks. The heuristics based in the 3D world coordinates are as follows:
<ol>
  <li>Given the maximum distance of the LiDAR sensors, we ensured the visible environment was empty except for the table and mounting block for the robot.</li>
  <li>LiDAR readings and joint angles were recorded while the Panda moved to random locations within the virtual wall constraints.</li>
  <li>All data where LiDAR sensor distances exceeded 60 cm were labeled as <code>ENVIRONMENT</code>.</li>
  <li>All data where the calculated 3D height was below 0 cm was the table and hence labeled as <code>ENVIRONMENT</code>.</li>
  <li>All remaining data was labeled as <code>SELF</code>.</li>
</ol></p>
<p>Classification was achieved using sequential feedforward networks with three dense layers of size 7, a dropout layer and a final layer with sigmoid activation function. During training the class weights were balanced since seeing itself while moving within the virtual walls was a rare event and in steep minority in comparison to events when it did not see itself. Without this, the network simply learnt to never predict seeing itself.</p>
<p>
  <div class="row uniform">
    <div class="3u"></div>
    <div class="6u 12u$(small)">
      <figure class="cap-left">    
        <img class="image fit" src='https://argmax.ai/images/dlrcpandas/lidar_supervised_model.png' alt='Feedforward Neural Network for Supervised LiDAR Classification.'>
        <figcaption> 
          Feedforward Neural Network for Supervised LiDAR Classification.
        </figcaption>
      </figure>
    </div>
    <div class="3u$"></div>
  </div>
</p>

<p>For this approach to be successful, it was crucial that the environment was empty, anything detected closer than 60 cm and above table level was classified as self. The figures below show the clear and consistent peaks in the distribution of LiDAR readings in an empty environment. In addition, the training data was augmented with LiDAR readings artificially shortened to represent objects in the environment, enabling the network to learn rules and generalise to a cluttered environment. Later data was collected where particular LiDAR sensors were obstructed and used in combination with other data to train final versions of the neural network classifiers.</p>
<figure class="cap-left">    
  <img class="image fit" src='https://argmax.ai/images/dlrcpandas/lidar-hist.png' alt='Histogram of LiDAR Readings for Each Sensor'>
  <figcaption> 
    Figure showing histograms of LiDAR readings for each sensor. The LiDAR values for all randomly selected joint configurations are separated into bins of 20cm. A vast proportion of regions fall into the 2m bin as all values which fall outside the detectable region are capped at 2m. This is not illustrated clearly in these figures as for visibility the vertical axis limit is restricted to 1000.
  </figcaption>
</figure>

<p>Pearson Correlation Coefficients were calculated to test the association between LiDAR sensors. A value of zero implies no correlation and 1 strongly correlated. As the maximum value is around 0.3, there appears to be little or no correlation between LiDAR sensors. Thus it made sense to treat LiDARs separately and model with separate neural networks.</p>
<p>
  <div class="row uniform">
    <div class="3u"></div>
    <div class="6u 12u$(small)">
      <figure class="cap-left">    
        <img class="image fit" src='https://argmax.ai/images/dlrcpandas/lidar_corr.jpeg' alt='LiDAR Sensor Correlation'>
        <figcaption> 
          Figure showing Pearson Correlation Coefficients for LiDAR data.
        </figcaption>
      </figure>
    </div>
    <div class="3u$"></div>
  </div>
</p>

<h3 id="unsupervised-prediction-of-lidar-readings">Unsupervised Prediction of LiDAR readings</h3>
<p>Another set of feed forward networks were trained for each sensor to predict the mean of the expected LiDAR reading in the absence of objects, assuming that the data is normally distributed with a variance of 1. The inputs to the network are the 7 joint angles, training continued over 100 epochs with the learning rate decaying over the training period, maximising the likelihood of achieving the ground truth LiDAR value given the predicted mean and an assumed variance of 1.</p>
<p>Observing LiDAR output in a constant joint angle configuration showed several significant outliers over time. In order to ensure the accuracy of the training data when learning to predict actual LiDAR readings, batches of LiDAR data were recorded and the median of these was taken to represent the expected, ground truth LiDAR reading for each joint configuration randomly selected from within the virtual walls.</p>
<p>
  <div class="row uniform">
    <div class="3u"></div>
    <div class="6u 12u$(small)">
      <figure class="cap-left">    
        <img class="image fit" src='https://argmax.ai/images/dlrcpandas/nn_median_ss_7.png' alt='Typical Feedforward Neural Network for LiDAR Prediction.'>
        <figcaption> 
          Feedforward neural network for predicting LiDAR readings in the default environment. Using this we can deduce the presence of unexpected objects.
        </figcaption>
      </figure>
    </div>
    <div class="3u$"></div>
  </div>
</p>

<h3 id="results">Results</h3>
<p>The figure below shows the results of visualising the classification and LiDAR reading prediction. There is a high degree of accuracy when the end effector is within the virtual walls, where training data was collected. As anticipated, a lower level of accuracy is achieved outside the virtual walls, however it is still able to approximate readings and predict values within around 20cm. Predicting self using the classified model was tested and trained in awkward positions where it achieves an acceptable level of accuracy.</p>
<figure class="cap-left">    
  <img class="image fit" src='https://argmax.ai/images/dlrcpandas/lidar_classification_results.png' alt='LiDAR classification'>
  <figcaption> 
    Screen shot of live demonstration showing LiDAR classification and prediction. Classification is depicted using colour where light blue is `ENVIRONMENT`, green is `SELF` and orange is `OTHER`. Live LiDAR data is the left hand column and the predicted values adjacent, one pair for each LiDAR sensor. In this situation LiDAR 0 was obstructed and the model correctly distinguished it as an unexpected object. All other LiDAR predictions are within acceptable limits.
  </figcaption>
</figure>

<p>When testing the model on the real robot we noticed that it worked well for the usual configurations, but did not perform so well in more awkward configurations. This can be explained by the lack of training data in these configurations. We improved the model by added some more of the unusual cases.</p>
<h2 id="collision-avoidance">Collision Avoidance</h2>
<h3 id="joint-control">Joint Control</h3>
<p>The Panda has two basic control modes. The first is POSITION CONTROL, in which the user provides a desired end-effector position in cartesian coordinates and optionally a quaternion-orientation. An internal controller is used to move to this target position. The second possibility is JOINT CONTROL. For this control you either send joint positions or torques for each joint. For our collision avoidance it was necessary to control the robot in joint space which required that we write a torque controller for the joints.</p>
<h3 id="nullspace">Nullspace</h3>
<p>Collision avoidance means doing two tasks at once, reaching a goal state while at the same time avoiding obstacles. In our approach these obstacles are detected by the seven LiDAR sensors distributed on the robot joints. This is achieved by computing the torques to reach the target position while conducting collision avoidance in the nullspace of the joints. This means that even though the robot has reached the target position with its end-effector it still has degrees of freedom available in the joints. The joints can then be moved without moving away from the target position. Since both tasks cannot be executed with the same accuracy priorities are defined for the tasks. The task with the lower priority will then be executed with a lower accuracy. For the prioritisation we have used Bayesian task prioritisation as defined in <a href='#paraschos2017probabilistic' id='ref-paraschos2017probabilistic-1'>Paraschos et al. (2017)</a>. This allows the computation of the combined torque control signal for multiple tasks, which in our case is reaching a target position and avoiding obstacles seen by the LiDAR sensors.</p>
<h3 id="obstacles">Obstacles</h3>
<p>In our approach we consider obstacles that are closer than 30 cm. Whenever a LiDAR sensor has a close reading it defines a repulsion field from this reading. The repulsion force depends on the distance from the obstacle, a closer obstacle results in a stronger force. The LiDAR sensors are always approximately along one axis of the respective joint coordinate system, thus the force is also only along that axis. The video below shows obstacle avoidance working.</p>
<table>
  <tr>
    <th>
        <video width=100% controls>
            <source src='https://argmax.ai/images/dlrcpandas/vid_obs_avoid.mp4' type="video/mp4" alt='Obstacle Avoidance Video' />
        </video>
    </th>
  </tr>
  <tbody>
    <tr>
      <td>Video showing the Panda avoiding obstacles simulated by blocking the LiDAR sensors.</td>
    </tr>
  </tbody>
</table>

<h3 id="messages">Messages</h3>
<p>Communication between the object detection and the control is via the internal messaging system. The object detection sends a target position to the torque control. The torque control then sends back commands to the gripper control once the target is reached.</p>
<h3 id="trouble-shooting">Trouble Shooting</h3>
<p>When implementing joint control in this way, several issues were overcome. We learnt that the controller is very sensitive to its parameters. Finding the correct parameters to give it enough torque so that it actually reaches the the target position but at the same time still reacts avoids obstacles with enough force was very difficult. Additionally a torque that might be sufficient to move from the initial position (go0) to a target position might not be sufficient to move from a different position to the same target. The parameters are especially hard to define when specifying the end-effector orientation in our target position in order to be able to grasp an object.</p>
<p>Our controller is designed on the principle of a coiled spring, the greater the distance to the goal point, the stronger the force that attracts it to this point. Since the force then decreases the closer we get to the target, we have found the need to increase the proportional gain of our controller once we are very close to the target position.</p>
<p>Controlling the torques directly presents a safety risk which was reduced by closely limiting the applied torques. We are restricting both the change in the torques with a rate limit and the actual maximum torque.</p>
<h2 id="task-object-detection">Task Object Detection</h2>
<p>To be able to perform our chosen "brownie" task it is necessary to distinguish Jenga blocks within the task environment. The Jenga blocks were chosen with have a distinct colour and they are all the same shape. The detection system uses the RGB and depth information to estimate a target position in the world coordinate frame as well as the orientation to be able to pick up the block.</p>
<h3 id="region-detection-using-colours">Region Detection using Colours</h3>
<p>Regions of colours matching the blocks are identified using HSV colour masks where the hue is separated from the saturation and value (a measure for darkness). For the three coloured blocks a range was determined to identify the blocks in a range of lighting conditions, while still excluding surroundings and shadows. This principle works very well in a regulated situation where the surroundings don't have the same colour as the blocks.</p>
<p>To remove the visual artifacts a filter is applied which removes all the blobs smaller than a threshold value. Size is estimated based on the coordinates of region bounding boxes in 3D world coordinates, and large items are ignored. In the case where blocks are close or overlapping, the blocks could be considered to be one region and the centroid of the region will no longer be centred on one block, but possibly the space between two blocks. This was considered ancillary to the central task of classifying sensors.</p>
<figure class="cap-left">    
  <img class="image fit" src='https://argmax.ai/images/dlrcpandas/object_detection.png' alt='Jenga Block Detection'>
  <figcaption> 
    Screenshot showing live Jenga block detection. The top row displays the RGB and depth images. The bottom left plot shows the segmentation of the RGB image highlighting the detected Jenga blocks. The bottom right plot shows the mask image with centroid labelled. The block labelled in red is the selected region with the required orientation marked with a red arrow.
  </figcaption>
</figure>

<h3 id="region-detection-using-depth-information">Region Detection using Depth Information</h3>
<p>Block detection would be more robust if depth information was incorporated, helping in situations where the background has a similar colour to that of the blocks. However, owing to the size of the Jenga blocks, there is a relatively small change in depth in contrast to other depth changes within the environment. Segmentation of the depth information was attempted using the Felzenswalbe algorithm <a href='#felzenszwalb2004efficient' id='ref-felzenszwalb2004efficient-1'>(Felzenszwalb and Huttenlocher, 2004)</a>. This worked well on large items, however when selecting small blocks, the small scale parameter resulted in over segmenting the table and even Jenga blocks. In cases when the table sloped away in the field of view, the sloping gradients resulted in striated regions. Strategies attempted to incorporate depth segmentation did not improve upon the HSV region detection.</p>
<h3 id="region-proposal">Region Proposal</h3>
<p>HSV colour masks are converted into separate regions using the Scikit Image library. These are supplemented with simple algorithms to estimate the size of objects once the bounding box and centroid of each region proposal are converted to coordinates in the camera frame using the camera's intrinsic matrix. The depth information obviates the need for a stereo camera reconstruction. These points are then further transformed to the 3D coordinates in the world frame relative to the base frame of the robot using the transformation matrices and the current robot joint angles.</p>
<p>We choose to pickup the object that is closest to the camera, based on the lowest distance read at the pixel location identified as the centroid of each region. This should make sure that we keep focussed on the same object while moving in to grab it. In practice this method does not guarantee the robot keeps moving to the same object because the camera has an offset to the gripper and therefor the closest object is subject to change. The robot would also not detect any changes to the blocks as no tracking has been implemented.</p>
<h3 id="orientation">Orientation</h3>
<p>It is important to know the orientation of the block to align the gripper to be able to grasp the object. We assume that the objects lie flat on the table so we only have to look for an angle in the table frame, grasping the object on its shortest axis. This axis is identified by establishing the edge points of a chosen object and then selecting the point closest to the centroid. The vector between this point and the centroid gives the required gripping direction.</p>
<h3 id="controlling-robot-to-achieve-task">Controlling robot to achieve task</h3>
<p>We use the built in position and orientation control from the Panda. To make sure the object is not hit from the side we first position the gripper right above the object. At this stage, the field of view is re-segmented, taking a closer look to better estimate the position and required orientation to successfully grasp the Jenga block.</p>
<table>
  <tr>
    <th>
        <video width=100% controls>
          <source src="https://argmax.ai/images/dlrcpandas/vid_task.mp4" type="video/mp4" alt='Panda completing Jenga clearing task.'>
        </video>
    </th>
  </tr>
  <tbody>
    <tr>
      <td>Video of Panda carrying out the Jenga clearing task.</td>
    </tr>
  </tbody>
</table>

<h3 id="future-work">Future Work</h3>
<p>There are various ways in which object detection could be improved for future work, including:
<ul>
  <li>Refine the regions by identifying rectangular shapes within region proposals to identify individual Jenga blocks.</li>
  <li>Track objects of interest.</li>
  <li>Make use of a neural network such as the pre-trained network YOLOv3 to detect task objects.</li>
</ul></p>
<h2 id="segmenting-image-data">Segmenting Image Data</h2>
<p>One of the objectives of the challenge was to classify image data pixel-wise which has been achieved for <code>SELF</code> segmenting RGB-D image information. A similar pixel-wise approach was taken to classify <code>OTHER</code> using motion as discussed in the next section.</p>
<h3 id="segnet">SegNet</h3>
<p>SegNet is a convolutional autoencoder developed by members of the Computer Vision and Robotics Group at the University of Cambridge <a href='#badrinarayanan2015segnet' id='ref-badrinarayanan2015segnet-2'>(Badrinarayanan et al., 2015)</a>. The figure below shows the network architecture. The network learns to predict pixel-wise classification of images through supervised learning.</p>
<figure class="cap-left">    
  <img class="image fit" src='https://argmax.ai/images/dlrcpandas/segnet-architecture.png' alt='SegNet Network Architecture'>
  <figcaption> 
    Figure showing SegNet Network Architecture. Credit: <a href="arxiv/abs/1511.00561">arxiv/abs/1511.00561</a>.
  </figcaption>
</figure>

<h3 id="creating-training-masks">Creating training masks</h3>
<p>The basis of automatically creating the training masks is done in the same fashion as with the Lidars. In this case the depth camera can be seen as a large collection of lidar sensors. The depth image is transformed into a 3d map of pixels in the frame of the base of the robot. Everything that is far away is removed just like everything below the table level. The remaining pixels are transformed back to the pixel frame which gives a mask image of the robot. The masks were further refined using colour masking to remove areas where the base of the robot was included in the mask, examples are shown below. RGB images were downsampled to ensure the same resolution across all images. Depth images were converted to single channel grayscale encoding.</p>
<p>
  <div class="row uniform">
    <div class="4u 10u$(small)">
      <figure class="cap-left">    
        <img class="image fit" src='https://argmax.ai/images/dlrcpandas/all-10-12-Y-15-0006-rgb.png' alt='RGB'>
        <figcaption> 
          RGB Image
        </figcaption>
      </figure>
    </div>
    <div class="4u 10u$(small)">
      <figure class="cap-left">    
        <img class="image fit" src='https://argmax.ai/images/dlrcpandas/all-10-12-Y-15-0006-mask.png' alt='Mask'>
        <figcaption> 
          Mask Image
        </figcaption>
      </figure>
    </div>
    <div class="4u 10u$(small)">
      <figure class="cap-left">    
        <img class="image fit" src='https://argmax.ai/images/dlrcpandas/all-10-12-Y-15-0006-depth-map.png' alt='Depth'>
        <figcaption> 
          Depth Gray Scale
        </figcaption>
      </figure>
    </div>
  </div>
</p>

<h3 id="multimodal-learning">Multimodal Learning</h3>
<p>Images were collected to ensure an adequate proportion of images with the Panda in the field of view of the camera. These were scaled ensuring an upper limit of 1.0 for the RGB-D channels and used to train the SegNet network without using any pre-trained weights.</p>
<p>Results using only the RGB data were susceptible to classifying other fixtures in the environment falsely as the Panda. To counter this, a parallel SegNet architecture was implemented to process the depth information, similar to an approach taken by <a href='#eitel2015multimodal' id='ref-eitel2015multimodal-1'>Eitel et al. (2015)</a>. The two networks were concatenated and combined with a final fusion layer before predicting the pixel-wise one hot encoding of not self and self.</p>
<figure class="cap-left">    
  <img class="image fit" src='https://argmax.ai/images/dlrcpandas/camera_self_classification.png' alt='Screenshot of live demonstration.'>
  <figcaption> 
    Figure showing live segmentation of RGB-D feed. The top row displays the raw RGB and depth images, the bottom left displays a heat map of the probability that the pixel is `SELF`. The plot on the bottom right displays the mask constructed of the maximum of the probability of `SELF` vs not `SELF`.
  </figcaption>
</figure>

<p>Pixelwise segmentation is demonstrated in the visualisation of the Panda field of view as shown above. The RGB network learnt features based on colour and texture, where as the depth network learnt features based on shape, particularly the distinctive curvature of the elbow joint. As to be expected, it can be fooled by objects exhibiting similar features.</p>
<h3 id="future-work_1">Future Work</h3>
<p>It would be more beneficial if the depth and RGB information could be learnt in the context of the current configuration of the robot, specifically the joint angles of the robot. The Panda may then be able to learn to distinguish between itself and another Panda, or even another object with properties similar to the features that have been learnt by the network. To achieve this, the joint angles could be incorporated as inputs after the encoder to ensure sufficient emphasis is placed on the relatively few inputs representing the state of the robot.</p>
<h2 id="identifying-other-agents">Identifying Other agents</h2>
<p>Identifying other agents in the environment is an important aspect of collaborating robots. In our approach another robot is defined by the fact that it is moving. A stationary robot can be treated as any other object in the environment. Once it starts moving however the robot has to be a lot more careful around it and as a later step maybe even plan its trajectory differently. This is why we wanted to be able to detect motion in the environment. We have then used a pre-trained neural network called FlowNet2 <a href='#ilg2017flownet' id='ref-ilg2017flownet-2'>(Ilg et al., 2017)</a> developed by Freiburg to detect the movement between to camera frames.</p>
<p>The video below shows the detected flow of a moving robot. On the left you can see the Panda picking up objects and on the right the detected flow, which can be interpreted as the velocity in x and y direction. To visualize it the output flow is graded by both magnitude and direction of velocity.</p>
<table>
  <tr>
    <th>
        <video width=100% controls>
          <source src="https://argmax.ai/images/dlrcpandas/vid_stable_robot_flow.mp4" type="video/mp4" alt="flow net video">
        </video>
    </th>
  </tr>
  <tbody>
    <tr>
      <td>Video showing the detection of motion whilst the Panda is picking up blocks.</td>
    </tr>
  </tbody>
</table>

<p>One disadvantage of FlowNet2 is that it can only detect movement if the camera itself is stationary. Deducting the camera movement from the frames as a preprocessing step might be possible for small movements of the camera but is not feasible in our scenario.</p>
<p>This means that we would have to stop the robots movement every now and again to scan the environment for moving objects. If the robot detects a moving object close to its next target position it should then plan its movement slower and stop more often to check for moving objects.</p>
<p>Even though the detection of movement works quite well for us, we have not yet integrated this classification into our brownie tasks since we are not collaborating with another robot yet. For future work it would be very interesting to try to add as a part of the collision avoidance.</p>
<h2 id="conclusions-and-future-work">Conclusions and Future Work</h2>
<p>While completing a task with the Panda we have been successful in using deep learning to classify <code>SELF</code> pixel-wise in images using SegNet, as well as with each of the 9 LiDAR sensors using a combination of feedforward neural networks. We are able to estimate live LiDAR readings anticipated within the default <code>ENVIRONMENT</code>. Based on this information, we can deduce <code>OTHER</code> objects in the environment. Future work includes combining the task and joint control modes into one controller, where this classification can be used to inform collision avoidance. In the absence of knowing <code>SELF</code>, the Panda would attempt to avoid itself resulting in unpredictable behaviour.</p>
<p>Detecting movement provides a strategy for detecting <code>OTHER</code> agents. We are able to detect motion in the field of view of a stationary end effector using FlowNet2 returning pixel wise classification of <code>OTHER</code> based on movement. This was demonstrated using footage of our Panda completing a task to show how this can be used in practise. Future work includes incorporating this into trajectory planning to the point where collaborating robots could apply turn taking when completing a task. There are additional demonstrations using footage from the Panda depth camera.</p><hr>
<h2>Bibliography</h2>
<p id='badrinarayanan2015segnet'>Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla.
Segnet: a deep convolutional encoder-decoder architecture for image segmentation.
<em>arXiv preprint arXiv:1511.00561</em>, 2015. <a class="cite-backref" href="#ref-badrinarayanan2015segnet-1" title="Jump back to reference 1">↩</a><a class="cite-backref" href="#ref-badrinarayanan2015segnet-1" title="Jump back to reference 1"> <sup>1</sup> </a><a class="cite-backref" href="#ref-badrinarayanan2015segnet-2" title="Jump back to reference 2"><sup>2</sup> </a></p>
<p id='eitel2015multimodal'>Andreas Eitel, Jost&nbsp;Tobias Springenberg, Luciano Spinello, Martin Riedmiller, and Wolfram Burgard.
Multimodal deep learning for robust rgb-d object recognition.
In <em>Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on</em>, 681–687. IEEE, 2015. <a class="cite-backref" href="#ref-eitel2015multimodal-1" title="Jump back to reference 1">↩</a></p>
<p id='felzenszwalb2004efficient'>Pedro&nbsp;F Felzenszwalb and Daniel&nbsp;P Huttenlocher.
Efficient graph-based image segmentation.
<em>International journal of computer vision</em>, 59(2):167–181, 2004. <a class="cite-backref" href="#ref-felzenszwalb2004efficient-1" title="Jump back to reference 1">↩</a></p>
<p id='ilg2017flownet'>Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox.
Flownet 2.0: evolution of optical flow estimation with deep networks.
In <em>IEEE conference on computer vision and pattern recognition (CVPR)</em>, volume&nbsp;2, 6. 2017. <a class="cite-backref" href="#ref-ilg2017flownet-1" title="Jump back to reference 1">↩</a><a class="cite-backref" href="#ref-ilg2017flownet-1" title="Jump back to reference 1"> <sup>1</sup> </a><a class="cite-backref" href="#ref-ilg2017flownet-2" title="Jump back to reference 2"><sup>2</sup> </a></p>
<p id='paraschos2017probabilistic'>Alexandros Paraschos, Rudolf Lioutikov, Jan Peters, Gerhard Neumann, and others.
Probabilistic prioritization of movement primitives.
<em>IEEE Robotics and Automation Letters</em>, 2017. <a class="cite-backref" href="#ref-paraschos2017probabilistic-1" title="Jump back to reference 1">↩</a></p>

    </p>

    <footer>
      <ul class="stats">
        <!-- <li id='article_category'><a href="https://argmax.ai/blog/">blog</a></li> -->


          <li><a target="_blank" href='mailto:?subject=Sharing a post from argmax.ai&body=Check out this post I came across "DLRC 2018 winners!" at https://argmax.ai/showmenot/dlrc-18-winners/"' class="icon fa-envelope">Email</a></li>
          <li><a target="_blank" href='http://twitter.com/intent/tweet?status=DLRC 2018 winners!+https://argmax.ai/showmenot/dlrc-18-winners/' class="icon fa-twitter">Twitter</a></li>
          <li><a target="_blank" href='http://www.reddit.com/submit?url=https://argmax.ai/showmenot/dlrc-18-winners/&title=DLRC 2018 winners!' class="icon fa-reddit">Reddit</a></li>
          <li><a target="_blank" href="http://www.linkedin.com/shareArticle?mini=true&url=https://argmax.ai/showmenot/dlrc-18-winners/&title=DLRC 2018 winners!&source=https://argmax.ai" class="icon fa-linkedin">LinkedIn</a></li>
          <li><a target="_blank" href='https://news.ycombinator.com/submitlink?t=DLRC 2018 winners!&u=https://argmax.ai/showmenot/dlrc-18-winners/' class="icon fa-hacker-news">HackerNews</a></li>

      </ul>
    </footer>
  </article>

  <ul class="actions pagination">

      <li><a href="#" class="disabled button previous">Previous post</a></li>

      <li><a href="#" class="disabled button next">Next post</a></li>
  </ul>
  
</div>






<div id="comments" class="comments">

</div>

<section id="footer">
  <p class="copyright">&copy; argmax.ai  - <a href="https://argmax.ai//imprint">Imprint</a> - <a href="https://argmax.ai//data-protection">Data Protection</a> - <a href="https://argmax.ai//copyright">Copyright</a>
</section><!-- <section id="footer">
  <p class="copyright">&copy; argmax.ai. 
  Powered by <a href="http://blog.getpelican.com/">Pelican</a>.</p>
</section> -->

		</div>

	<!-- Scripts -->
		<script src="https://argmax.ai/theme/js/skel.min.js"></script>
		<script src="https://argmax.ai/theme/js/util.js"></script>
		<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
		<script src="https://argmax.ai/theme/js/main.js"></script>

	</body>
</html>